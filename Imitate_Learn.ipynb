{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import gym\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import imageio\n",
    "\n",
    "import IPython\n",
    "from IPython.display import Image\n",
    "\n",
    "import sys, subprocess\n",
    "import time\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "from State import State\n",
    "from Movement_evaluation import evaluate_by_gravity\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from Ball import Ball\n",
    "from Config import *\n",
    "from Game import Game\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"LZM_1\", \"LZM_2\", \"ZKH1\", \"ZKH2\", \"PHL\", \"FYJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game(screen_x, screen_y, end_line, balls_setting, max_random_ball_level)\n",
    "N_features = game.current_state.vectorize().shape[0]\n",
    "N_sample = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros( (N_sample, N_features ) )\n",
    "Y_train = np.zeros( N_sample )\n",
    "current_row = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    X_np = np.loadtxt(\"data/X_train_\" + name + \".csv\", delimiter=\",\")\n",
    "    Y_np = np.loadtxt(\"data/Y_train_\" + name + \".csv\", delimiter=\",\")\n",
    "    for x, y in zip(X_np, Y_np):\n",
    "        X_train[current_row, :] = x\n",
    "        Y_train[current_row] = y\n",
    "        current_row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train[:current_row]\n",
    "Y = Y_train[:current_row]\n",
    "Y = Y / screen_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y= np.clip(Y, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'percentage')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(Y, density=True, color='orange', bins=20)\n",
    "plt.ylabel('actions')\n",
    "plt.xlabel('percentage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitate Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgr_lr = LinearRegression().fit(X_train, y_train)\n",
    "y_predict = rgr_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1529211478507719"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_predict, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation = \"relu\"))\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=\"mae\",\n",
    "              metrics=['mae'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "34/34 [==============================] - 1s 8ms/step - loss: 0.3781 - mae: 0.3781 - val_loss: 0.3786 - val_mae: 0.3786\n",
      "Epoch 2/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.3529 - mae: 0.3529 - val_loss: 0.3701 - val_mae: 0.3701\n",
      "Epoch 3/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.3343 - mae: 0.3343 - val_loss: 0.3574 - val_mae: 0.3574\n",
      "Epoch 4/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.3253 - mae: 0.3253 - val_loss: 0.3409 - val_mae: 0.3409\n",
      "Epoch 5/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.3134 - mae: 0.3134 - val_loss: 0.3462 - val_mae: 0.3462\n",
      "Epoch 6/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.3249 - mae: 0.3249 - val_loss: 0.3542 - val_mae: 0.3542\n",
      "Epoch 7/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.3005 - mae: 0.3005 - val_loss: 0.3447 - val_mae: 0.3447\n",
      "Epoch 8/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2956 - mae: 0.2956 - val_loss: 0.3417 - val_mae: 0.3417\n",
      "Epoch 9/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.3008 - mae: 0.3008 - val_loss: 0.3450 - val_mae: 0.3450\n",
      "Epoch 10/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2933 - mae: 0.2933 - val_loss: 0.3398 - val_mae: 0.3398\n",
      "Epoch 11/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2840 - mae: 0.2840 - val_loss: 0.3444 - val_mae: 0.3444\n",
      "Epoch 12/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2921 - mae: 0.2921 - val_loss: 0.3507 - val_mae: 0.3507\n",
      "Epoch 13/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2905 - mae: 0.2905 - val_loss: 0.3374 - val_mae: 0.3374\n",
      "Epoch 14/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2861 - mae: 0.2861 - val_loss: 0.3387 - val_mae: 0.3387\n",
      "Epoch 15/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2750 - mae: 0.2750 - val_loss: 0.3456 - val_mae: 0.3456\n",
      "Epoch 16/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2691 - mae: 0.2691 - val_loss: 0.3333 - val_mae: 0.3333\n",
      "Epoch 17/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2597 - mae: 0.2597 - val_loss: 0.3338 - val_mae: 0.3338\n",
      "Epoch 18/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2658 - mae: 0.2658 - val_loss: 0.3294 - val_mae: 0.3294\n",
      "Epoch 19/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2620 - mae: 0.2620 - val_loss: 0.3524 - val_mae: 0.3524\n",
      "Epoch 20/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2694 - mae: 0.2694 - val_loss: 0.3240 - val_mae: 0.3240\n",
      "Epoch 21/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2563 - mae: 0.2563 - val_loss: 0.3272 - val_mae: 0.3272\n",
      "Epoch 22/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2596 - mae: 0.2596 - val_loss: 0.3365 - val_mae: 0.3365\n",
      "Epoch 23/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2499 - mae: 0.2499 - val_loss: 0.3278 - val_mae: 0.3278\n",
      "Epoch 24/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2427 - mae: 0.2427 - val_loss: 0.3256 - val_mae: 0.3256\n",
      "Epoch 25/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2458 - mae: 0.2458 - val_loss: 0.3303 - val_mae: 0.3303\n",
      "Epoch 26/200\n",
      "34/34 [==============================] - 1s 20ms/step - loss: 0.2448 - mae: 0.2448 - val_loss: 0.3236 - val_mae: 0.3236\n",
      "Epoch 27/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2443 - mae: 0.2443 - val_loss: 0.3267 - val_mae: 0.3267\n",
      "Epoch 28/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2418 - mae: 0.2418 - val_loss: 0.3247 - val_mae: 0.3247\n",
      "Epoch 29/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2359 - mae: 0.2359 - val_loss: 0.3268 - val_mae: 0.3268\n",
      "Epoch 30/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2342 - mae: 0.2342 - val_loss: 0.3297 - val_mae: 0.3297\n",
      "Epoch 31/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2209 - mae: 0.2209 - val_loss: 0.3282 - val_mae: 0.3282\n",
      "Epoch 32/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2167 - mae: 0.2167 - val_loss: 0.3222 - val_mae: 0.3222\n",
      "Epoch 33/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2262 - mae: 0.2262 - val_loss: 0.3144 - val_mae: 0.3144\n",
      "Epoch 34/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2010 - mae: 0.2010 - val_loss: 0.3185 - val_mae: 0.3185\n",
      "Epoch 35/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2100 - mae: 0.2100 - val_loss: 0.3258 - val_mae: 0.3258\n",
      "Epoch 36/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2213 - mae: 0.2213 - val_loss: 0.3226 - val_mae: 0.3226\n",
      "Epoch 37/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2232 - mae: 0.2232 - val_loss: 0.3280 - val_mae: 0.3280\n",
      "Epoch 38/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2229 - mae: 0.2229 - val_loss: 0.3312 - val_mae: 0.3312\n",
      "Epoch 39/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2152 - mae: 0.2152 - val_loss: 0.3150 - val_mae: 0.3150\n",
      "Epoch 40/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2103 - mae: 0.2103 - val_loss: 0.3369 - val_mae: 0.3369\n",
      "Epoch 41/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2162 - mae: 0.2162 - val_loss: 0.3180 - val_mae: 0.3180\n",
      "Epoch 42/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1905 - mae: 0.1905 - val_loss: 0.3188 - val_mae: 0.3188\n",
      "Epoch 43/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1991 - mae: 0.1991 - val_loss: 0.3113 - val_mae: 0.3113\n",
      "Epoch 44/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1985 - mae: 0.1985 - val_loss: 0.3113 - val_mae: 0.3113\n",
      "Epoch 45/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1972 - mae: 0.1972 - val_loss: 0.3204 - val_mae: 0.3204\n",
      "Epoch 46/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2041 - mae: 0.2041 - val_loss: 0.3275 - val_mae: 0.3275\n",
      "Epoch 47/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1926 - mae: 0.1926 - val_loss: 0.3284 - val_mae: 0.3284\n",
      "Epoch 48/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1858 - mae: 0.1858 - val_loss: 0.3266 - val_mae: 0.3266\n",
      "Epoch 49/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1817 - mae: 0.1817 - val_loss: 0.3282 - val_mae: 0.3282\n",
      "Epoch 50/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1881 - mae: 0.1881 - val_loss: 0.3281 - val_mae: 0.3281\n",
      "Epoch 51/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1847 - mae: 0.1847 - val_loss: 0.3203 - val_mae: 0.3203\n",
      "Epoch 52/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1869 - mae: 0.1869 - val_loss: 0.3203 - val_mae: 0.3203\n",
      "Epoch 53/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1957 - mae: 0.1957 - val_loss: 0.3266 - val_mae: 0.3266\n",
      "Epoch 54/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1919 - mae: 0.1919 - val_loss: 0.3213 - val_mae: 0.3213\n",
      "Epoch 55/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1774 - mae: 0.1774 - val_loss: 0.3195 - val_mae: 0.3195\n",
      "Epoch 56/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1869 - mae: 0.1869 - val_loss: 0.3173 - val_mae: 0.3173\n",
      "Epoch 57/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1779 - mae: 0.1779 - val_loss: 0.3287 - val_mae: 0.3287\n",
      "Epoch 58/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1748 - mae: 0.1748 - val_loss: 0.3176 - val_mae: 0.3176\n",
      "Epoch 59/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1747 - mae: 0.1747 - val_loss: 0.3228 - val_mae: 0.3228\n",
      "Epoch 60/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1764 - mae: 0.1764 - val_loss: 0.3201 - val_mae: 0.3201\n",
      "Epoch 61/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1698 - mae: 0.1698 - val_loss: 0.3122 - val_mae: 0.3122\n",
      "Epoch 62/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1679 - mae: 0.1679 - val_loss: 0.3302 - val_mae: 0.3302\n",
      "Epoch 63/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1772 - mae: 0.1772 - val_loss: 0.3111 - val_mae: 0.3111\n",
      "Epoch 64/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1631 - mae: 0.1631 - val_loss: 0.3230 - val_mae: 0.3230\n",
      "Epoch 65/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1652 - mae: 0.1652 - val_loss: 0.3207 - val_mae: 0.3207\n",
      "Epoch 66/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1675 - mae: 0.1675 - val_loss: 0.3212 - val_mae: 0.3212\n",
      "Epoch 67/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1643 - mae: 0.1643 - val_loss: 0.3131 - val_mae: 0.3131\n",
      "Epoch 68/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1603 - mae: 0.1603 - val_loss: 0.3146 - val_mae: 0.3146\n",
      "Epoch 69/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1569 - mae: 0.1569 - val_loss: 0.3100 - val_mae: 0.3100\n",
      "Epoch 70/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1712 - mae: 0.1712 - val_loss: 0.3171 - val_mae: 0.3171\n",
      "Epoch 71/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1727 - mae: 0.1727 - val_loss: 0.3147 - val_mae: 0.3147\n",
      "Epoch 72/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1600 - mae: 0.1600 - val_loss: 0.3105 - val_mae: 0.3105\n",
      "Epoch 73/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1569 - mae: 0.1569 - val_loss: 0.3155 - val_mae: 0.3155\n",
      "Epoch 74/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1548 - mae: 0.1548 - val_loss: 0.3155 - val_mae: 0.3155\n",
      "Epoch 75/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1521 - mae: 0.1521 - val_loss: 0.3118 - val_mae: 0.3118\n",
      "Epoch 76/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1574 - mae: 0.1574 - val_loss: 0.3095 - val_mae: 0.3095\n",
      "Epoch 77/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1547 - mae: 0.1547 - val_loss: 0.3090 - val_mae: 0.3090\n",
      "Epoch 78/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1508 - mae: 0.1508 - val_loss: 0.3164 - val_mae: 0.3164\n",
      "Epoch 79/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1501 - mae: 0.1501 - val_loss: 0.3177 - val_mae: 0.3177\n",
      "Epoch 80/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1527 - mae: 0.1527 - val_loss: 0.3139 - val_mae: 0.3139\n",
      "Epoch 81/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1459 - mae: 0.1459 - val_loss: 0.3161 - val_mae: 0.3161\n",
      "Epoch 82/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1400 - mae: 0.1400 - val_loss: 0.3199 - val_mae: 0.3199\n",
      "Epoch 83/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1397 - mae: 0.1397 - val_loss: 0.3111 - val_mae: 0.3111\n",
      "Epoch 84/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1554 - mae: 0.1554 - val_loss: 0.3128 - val_mae: 0.3128\n",
      "Epoch 85/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1399 - mae: 0.1399 - val_loss: 0.3141 - val_mae: 0.3141\n",
      "Epoch 86/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1344 - mae: 0.1344 - val_loss: 0.3132 - val_mae: 0.3132\n",
      "Epoch 87/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1397 - mae: 0.1397 - val_loss: 0.3145 - val_mae: 0.3145\n",
      "Epoch 88/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1358 - mae: 0.1358 - val_loss: 0.3074 - val_mae: 0.3074\n",
      "Epoch 89/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1366 - mae: 0.1366 - val_loss: 0.3103 - val_mae: 0.3103\n",
      "Epoch 90/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1317 - mae: 0.1317 - val_loss: 0.3168 - val_mae: 0.3168\n",
      "Epoch 91/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1385 - mae: 0.1385 - val_loss: 0.3098 - val_mae: 0.3098\n",
      "Epoch 92/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1349 - mae: 0.1349 - val_loss: 0.3112 - val_mae: 0.3112\n",
      "Epoch 93/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1433 - mae: 0.1433 - val_loss: 0.3129 - val_mae: 0.3129\n",
      "Epoch 94/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1340 - mae: 0.1340 - val_loss: 0.3177 - val_mae: 0.3177\n",
      "Epoch 95/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1504 - mae: 0.1504 - val_loss: 0.3219 - val_mae: 0.3219\n",
      "Epoch 96/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1400 - mae: 0.1400 - val_loss: 0.3168 - val_mae: 0.3168\n",
      "Epoch 97/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1268 - mae: 0.1268 - val_loss: 0.3173 - val_mae: 0.3173\n",
      "Epoch 98/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1396 - mae: 0.1396 - val_loss: 0.3216 - val_mae: 0.3216\n",
      "Epoch 99/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1328 - mae: 0.1328 - val_loss: 0.3123 - val_mae: 0.3123\n",
      "Epoch 100/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1409 - mae: 0.1409 - val_loss: 0.3140 - val_mae: 0.3140\n",
      "Epoch 101/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1331 - mae: 0.1331 - val_loss: 0.3121 - val_mae: 0.3121\n",
      "Epoch 102/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1246 - mae: 0.1246 - val_loss: 0.3177 - val_mae: 0.3177\n",
      "Epoch 103/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1318 - mae: 0.1318 - val_loss: 0.3241 - val_mae: 0.3241\n",
      "Epoch 104/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1279 - mae: 0.1279 - val_loss: 0.3133 - val_mae: 0.3133\n",
      "Epoch 105/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1327 - mae: 0.1327 - val_loss: 0.3249 - val_mae: 0.3249\n",
      "Epoch 106/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1305 - mae: 0.1305 - val_loss: 0.3197 - val_mae: 0.3197\n",
      "Epoch 107/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1230 - mae: 0.1230 - val_loss: 0.3231 - val_mae: 0.3231\n",
      "Epoch 108/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1292 - mae: 0.1292 - val_loss: 0.3129 - val_mae: 0.3129\n",
      "Epoch 109/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1272 - mae: 0.1272 - val_loss: 0.3135 - val_mae: 0.3135\n",
      "Epoch 110/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1251 - mae: 0.1251 - val_loss: 0.3196 - val_mae: 0.3196\n",
      "Epoch 111/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1234 - mae: 0.1234 - val_loss: 0.3149 - val_mae: 0.3149\n",
      "Epoch 112/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1397 - mae: 0.1397 - val_loss: 0.3180 - val_mae: 0.3180\n",
      "Epoch 113/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1281 - mae: 0.1281 - val_loss: 0.3137 - val_mae: 0.3137\n",
      "Epoch 114/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1257 - mae: 0.1257 - val_loss: 0.3129 - val_mae: 0.3129\n",
      "Epoch 115/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1313 - mae: 0.1313 - val_loss: 0.3216 - val_mae: 0.3216\n",
      "Epoch 116/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1317 - mae: 0.1317 - val_loss: 0.3253 - val_mae: 0.3253\n",
      "Epoch 117/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1248 - mae: 0.1248 - val_loss: 0.3203 - val_mae: 0.3203\n",
      "Epoch 118/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1209 - mae: 0.1209 - val_loss: 0.3198 - val_mae: 0.3198\n",
      "Epoch 119/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1205 - mae: 0.1205 - val_loss: 0.3124 - val_mae: 0.3124\n",
      "Epoch 120/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1240 - mae: 0.1240 - val_loss: 0.3204 - val_mae: 0.3204\n",
      "Epoch 121/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1210 - mae: 0.1210 - val_loss: 0.3184 - val_mae: 0.3184\n",
      "Epoch 122/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1289 - mae: 0.1289 - val_loss: 0.3162 - val_mae: 0.3162\n",
      "Epoch 123/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1223 - mae: 0.1223 - val_loss: 0.3194 - val_mae: 0.3194\n",
      "Epoch 124/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1192 - mae: 0.1192 - val_loss: 0.3181 - val_mae: 0.3181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1223 - mae: 0.1223 - val_loss: 0.3128 - val_mae: 0.3128\n",
      "Epoch 126/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1201 - mae: 0.1201 - val_loss: 0.3201 - val_mae: 0.3201\n",
      "Epoch 127/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1074 - mae: 0.1074 - val_loss: 0.3146 - val_mae: 0.3146\n",
      "Epoch 128/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1117 - mae: 0.1117 - val_loss: 0.3153 - val_mae: 0.3153\n",
      "Epoch 129/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1203 - mae: 0.1203 - val_loss: 0.3200 - val_mae: 0.3200\n",
      "Epoch 130/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1169 - mae: 0.1169 - val_loss: 0.3144 - val_mae: 0.3144\n",
      "Epoch 131/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1138 - mae: 0.1138 - val_loss: 0.3177 - val_mae: 0.3177\n",
      "Epoch 132/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1219 - mae: 0.1219 - val_loss: 0.3174 - val_mae: 0.3174\n",
      "Epoch 133/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1117 - mae: 0.1117 - val_loss: 0.3125 - val_mae: 0.3125\n",
      "Epoch 134/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1059 - mae: 0.1059 - val_loss: 0.3161 - val_mae: 0.3161\n",
      "Epoch 135/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1137 - mae: 0.1137 - val_loss: 0.3143 - val_mae: 0.3143\n",
      "Epoch 136/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1137 - mae: 0.1137 - val_loss: 0.3120 - val_mae: 0.3120\n",
      "Epoch 137/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1099 - mae: 0.1099 - val_loss: 0.3208 - val_mae: 0.3208\n",
      "Epoch 138/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1054 - mae: 0.1054 - val_loss: 0.3132 - val_mae: 0.3132\n",
      "Epoch 139/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1281 - mae: 0.1281 - val_loss: 0.3109 - val_mae: 0.3109\n",
      "Epoch 140/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1112 - mae: 0.1112 - val_loss: 0.3210 - val_mae: 0.3210\n",
      "Epoch 141/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1104 - mae: 0.1104 - val_loss: 0.3283 - val_mae: 0.3283\n",
      "Epoch 142/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1163 - mae: 0.1163 - val_loss: 0.3194 - val_mae: 0.3194\n",
      "Epoch 143/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1117 - mae: 0.1117 - val_loss: 0.3158 - val_mae: 0.3158\n",
      "Epoch 144/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1108 - mae: 0.1108 - val_loss: 0.3121 - val_mae: 0.3121\n",
      "Epoch 145/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1013 - mae: 0.1013 - val_loss: 0.3225 - val_mae: 0.3225\n",
      "Epoch 146/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1020 - mae: 0.1020 - val_loss: 0.3194 - val_mae: 0.3194\n",
      "Epoch 147/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1107 - mae: 0.1107 - val_loss: 0.3188 - val_mae: 0.3188\n",
      "Epoch 148/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1227 - mae: 0.1227 - val_loss: 0.3173 - val_mae: 0.3173\n",
      "Epoch 149/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1131 - mae: 0.1131 - val_loss: 0.3284 - val_mae: 0.3284\n",
      "Epoch 150/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1165 - mae: 0.1165 - val_loss: 0.3238 - val_mae: 0.3238\n",
      "Epoch 151/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0964 - mae: 0.0964 - val_loss: 0.3080 - val_mae: 0.3080\n",
      "Epoch 152/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1148 - mae: 0.1148 - val_loss: 0.3139 - val_mae: 0.3139\n",
      "Epoch 153/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1071 - mae: 0.1071 - val_loss: 0.3091 - val_mae: 0.3091\n",
      "Epoch 154/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1077 - mae: 0.1077 - val_loss: 0.3102 - val_mae: 0.3102\n",
      "Epoch 155/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1060 - mae: 0.1060 - val_loss: 0.3151 - val_mae: 0.3151\n",
      "Epoch 156/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0990 - mae: 0.0990 - val_loss: 0.3071 - val_mae: 0.3071\n",
      "Epoch 157/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1039 - mae: 0.1039 - val_loss: 0.3090 - val_mae: 0.3090\n",
      "Epoch 158/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1077 - mae: 0.1077 - val_loss: 0.3103 - val_mae: 0.3103\n",
      "Epoch 159/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1073 - mae: 0.1073 - val_loss: 0.3074 - val_mae: 0.3074\n",
      "Epoch 160/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1069 - mae: 0.1069 - val_loss: 0.3105 - val_mae: 0.3105\n",
      "Epoch 161/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1059 - mae: 0.1059 - val_loss: 0.3128 - val_mae: 0.3128\n",
      "Epoch 162/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1117 - mae: 0.1117 - val_loss: 0.3073 - val_mae: 0.3073\n",
      "Epoch 163/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1159 - mae: 0.1159 - val_loss: 0.3142 - val_mae: 0.3142\n",
      "Epoch 164/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0969 - mae: 0.0969 - val_loss: 0.3162 - val_mae: 0.3162\n",
      "Epoch 165/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1077 - mae: 0.1077 - val_loss: 0.3121 - val_mae: 0.3121\n",
      "Epoch 166/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1026 - mae: 0.1026 - val_loss: 0.3135 - val_mae: 0.3135\n",
      "Epoch 167/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1051 - mae: 0.1051 - val_loss: 0.3124 - val_mae: 0.3124\n",
      "Epoch 168/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.0976 - mae: 0.0976 - val_loss: 0.3165 - val_mae: 0.3165\n",
      "Epoch 169/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.1028 - mae: 0.1028 - val_loss: 0.3115 - val_mae: 0.3115\n",
      "Epoch 170/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0991 - mae: 0.0991 - val_loss: 0.3102 - val_mae: 0.3102\n",
      "Epoch 171/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1064 - mae: 0.1064 - val_loss: 0.3145 - val_mae: 0.3145\n",
      "Epoch 172/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0930 - mae: 0.0930 - val_loss: 0.3087 - val_mae: 0.3087\n",
      "Epoch 173/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1008 - mae: 0.1008 - val_loss: 0.3140 - val_mae: 0.3140\n",
      "Epoch 174/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1042 - mae: 0.1042 - val_loss: 0.3103 - val_mae: 0.3103\n",
      "Epoch 175/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0997 - mae: 0.0997 - val_loss: 0.3202 - val_mae: 0.3202\n",
      "Epoch 176/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1039 - mae: 0.1039 - val_loss: 0.3212 - val_mae: 0.3212\n",
      "Epoch 177/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1023 - mae: 0.1023 - val_loss: 0.3088 - val_mae: 0.3088\n",
      "Epoch 178/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.0994 - mae: 0.0994 - val_loss: 0.3084 - val_mae: 0.3084\n",
      "Epoch 179/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1047 - mae: 0.1047 - val_loss: 0.3056 - val_mae: 0.3056\n",
      "Epoch 180/200\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.1026 - mae: 0.1026 - val_loss: 0.3183 - val_mae: 0.3183\n",
      "Epoch 181/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1001 - mae: 0.1001 - val_loss: 0.3150 - val_mae: 0.3150\n",
      "Epoch 182/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1042 - mae: 0.1042 - val_loss: 0.3132 - val_mae: 0.3132\n",
      "Epoch 183/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1034 - mae: 0.1034 - val_loss: 0.3156 - val_mae: 0.3156\n",
      "Epoch 184/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0986 - mae: 0.0986 - val_loss: 0.3153 - val_mae: 0.3153\n",
      "Epoch 185/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0955 - mae: 0.0955 - val_loss: 0.3112 - val_mae: 0.3112\n",
      "Epoch 186/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.0950 - mae: 0.0950 - val_loss: 0.3141 - val_mae: 0.3141\n",
      "Epoch 187/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1046 - mae: 0.1046 - val_loss: 0.3079 - val_mae: 0.3079\n",
      "Epoch 188/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1014 - mae: 0.1014 - val_loss: 0.3097 - val_mae: 0.3097\n",
      "Epoch 189/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1015 - mae: 0.1015 - val_loss: 0.3084 - val_mae: 0.3084\n",
      "Epoch 190/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0994 - mae: 0.0994 - val_loss: 0.3032 - val_mae: 0.3032\n",
      "Epoch 191/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0963 - mae: 0.0963 - val_loss: 0.3129 - val_mae: 0.3129\n",
      "Epoch 192/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1038 - mae: 0.1038 - val_loss: 0.3148 - val_mae: 0.3148\n",
      "Epoch 193/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0964 - mae: 0.0964 - val_loss: 0.3114 - val_mae: 0.3114\n",
      "Epoch 194/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0906 - mae: 0.0906 - val_loss: 0.3074 - val_mae: 0.3074\n",
      "Epoch 195/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0890 - mae: 0.0890 - val_loss: 0.3124 - val_mae: 0.3124\n",
      "Epoch 196/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0905 - mae: 0.0905 - val_loss: 0.3040 - val_mae: 0.3040\n",
      "Epoch 197/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0943 - mae: 0.0943 - val_loss: 0.3107 - val_mae: 0.3107\n",
      "Epoch 198/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0989 - mae: 0.0989 - val_loss: 0.3082 - val_mae: 0.3082\n",
      "Epoch 199/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0907 - mae: 0.0907 - val_loss: 0.3182 - val_mae: 0.3182\n",
      "Epoch 200/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1050 - mae: 0.1050 - val_loss: 0.3070 - val_mae: 0.3070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1de480af898>"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size = batch_size, validation_split=0.20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19821265675507713"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(model.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56801474],\n",
       "       [0.5594598 ],\n",
       "       [0.3381542 ],\n",
       "       [0.16388816],\n",
       "       [0.883243  ],\n",
       "       [0.27887172],\n",
       "       [0.5445646 ],\n",
       "       [0.02175206],\n",
       "       [0.7133662 ],\n",
       "       [0.04391927],\n",
       "       [0.5731407 ],\n",
       "       [0.3561979 ],\n",
       "       [0.3505982 ],\n",
       "       [0.9558308 ],\n",
       "       [0.87399995],\n",
       "       [0.3133912 ],\n",
       "       [0.27787715],\n",
       "       [0.36913446],\n",
       "       [0.10011596],\n",
       "       [0.03451426],\n",
       "       [0.7107775 ],\n",
       "       [0.0501123 ],\n",
       "       [0.8971641 ],\n",
       "       [0.66808754],\n",
       "       [0.85920626],\n",
       "       [0.8274144 ],\n",
       "       [0.77874887],\n",
       "       [0.03702336],\n",
       "       [0.5067528 ],\n",
       "       [0.3379709 ],\n",
       "       [0.5141749 ],\n",
       "       [0.77226233],\n",
       "       [0.02730657],\n",
       "       [0.3001306 ],\n",
       "       [0.02078642],\n",
       "       [0.599143  ],\n",
       "       [0.14171068],\n",
       "       [1.0294576 ],\n",
       "       [0.420514  ],\n",
       "       [0.07798386],\n",
       "       [0.58948255],\n",
       "       [0.13868776],\n",
       "       [0.17464715],\n",
       "       [0.11592299],\n",
       "       [0.03293159],\n",
       "       [0.30311504],\n",
       "       [0.69611365],\n",
       "       [0.7202698 ],\n",
       "       [0.827549  ],\n",
       "       [0.89942026],\n",
       "       [0.25321257],\n",
       "       [0.99903774],\n",
       "       [0.04258323],\n",
       "       [0.43010053],\n",
       "       [0.526882  ],\n",
       "       [1.0635841 ],\n",
       "       [0.4426672 ],\n",
       "       [0.03197992],\n",
       "       [0.36048934],\n",
       "       [0.20179233],\n",
       "       [0.04873033],\n",
       "       [0.68532455],\n",
       "       [0.14551157],\n",
       "       [0.06690578],\n",
       "       [0.4117855 ],\n",
       "       [0.5250245 ],\n",
       "       [0.93717164],\n",
       "       [0.70822954],\n",
       "       [0.23286828],\n",
       "       [0.02521126],\n",
       "       [0.10596712],\n",
       "       [0.12076409],\n",
       "       [0.24945089],\n",
       "       [0.57992435],\n",
       "       [0.49526098],\n",
       "       [0.49687043],\n",
       "       [0.20683512],\n",
       "       [0.6337366 ],\n",
       "       [0.42564917],\n",
       "       [0.7642373 ],\n",
       "       [0.09687692],\n",
       "       [0.93142986],\n",
       "       [0.03161702],\n",
       "       [0.75155103],\n",
       "       [0.03294328],\n",
       "       [0.8213829 ],\n",
       "       [0.43221545],\n",
       "       [0.02337255],\n",
       "       [0.27064526],\n",
       "       [0.09530249],\n",
       "       [0.50480175],\n",
       "       [0.6454265 ],\n",
       "       [1.0397145 ],\n",
       "       [0.11148456],\n",
       "       [0.03159311],\n",
       "       [0.33255762],\n",
       "       [0.29064056],\n",
       "       [0.11505429],\n",
       "       [0.41899258],\n",
       "       [0.19355895],\n",
       "       [0.1732918 ],\n",
       "       [0.12634864],\n",
       "       [0.8583381 ],\n",
       "       [0.49633145],\n",
       "       [0.03319728],\n",
       "       [1.0032305 ],\n",
       "       [0.28407302],\n",
       "       [0.05769987],\n",
       "       [0.73127884],\n",
       "       [0.06922909],\n",
       "       [0.02490204],\n",
       "       [0.43194306],\n",
       "       [0.04837985],\n",
       "       [0.02765641],\n",
       "       [0.5198058 ],\n",
       "       [0.22319993],\n",
       "       [0.06827787],\n",
       "       [0.43841505],\n",
       "       [0.5712069 ],\n",
       "       [0.786487  ],\n",
       "       [0.9753654 ],\n",
       "       [0.47227746],\n",
       "       [0.03093518],\n",
       "       [0.5322051 ],\n",
       "       [0.0733802 ],\n",
       "       [0.06688656],\n",
       "       [0.03481587],\n",
       "       [0.5809052 ],\n",
       "       [0.8496424 ],\n",
       "       [0.7360722 ],\n",
       "       [0.21000245],\n",
       "       [0.02727543],\n",
       "       [0.02412641],\n",
       "       [0.64641446],\n",
       "       [0.37737733],\n",
       "       [0.4702636 ],\n",
       "       [0.700747  ],\n",
       "       [0.7476995 ],\n",
       "       [0.1312645 ],\n",
       "       [0.99049485],\n",
       "       [0.8339266 ],\n",
       "       [0.08607122],\n",
       "       [0.05579967],\n",
       "       [0.32667485],\n",
       "       [0.05987851],\n",
       "       [0.6826284 ],\n",
       "       [0.04334076],\n",
       "       [0.5574167 ],\n",
       "       [0.5471079 ],\n",
       "       [0.29856214],\n",
       "       [0.3566863 ],\n",
       "       [0.62399226],\n",
       "       [0.15630627],\n",
       "       [0.26747355],\n",
       "       [0.9461793 ],\n",
       "       [1.044935  ],\n",
       "       [0.4679668 ],\n",
       "       [0.01754722],\n",
       "       [0.06043419],\n",
       "       [0.83242166],\n",
       "       [0.06087787],\n",
       "       [0.949126  ],\n",
       "       [0.07790904],\n",
       "       [0.02582982],\n",
       "       [0.53456306],\n",
       "       [0.15792704],\n",
       "       [0.03140121],\n",
       "       [0.07827029],\n",
       "       [0.7727112 ],\n",
       "       [0.2240362 ],\n",
       "       [0.23963188],\n",
       "       [0.9420017 ],\n",
       "       [0.05016713],\n",
       "       [0.4567621 ],\n",
       "       [0.10372919],\n",
       "       [0.0437545 ],\n",
       "       [0.1517427 ],\n",
       "       [0.18418902],\n",
       "       [0.731738  ],\n",
       "       [0.02458959],\n",
       "       [0.57531184],\n",
       "       [0.5577186 ],\n",
       "       [0.89084697],\n",
       "       [0.1777453 ],\n",
       "       [0.7318118 ],\n",
       "       [0.71506345],\n",
       "       [1.0098169 ],\n",
       "       [0.10354623],\n",
       "       [0.03393459],\n",
       "       [0.02435365],\n",
       "       [0.31316313],\n",
       "       [0.02519929],\n",
       "       [0.3001448 ],\n",
       "       [0.04264747],\n",
       "       [0.6826284 ],\n",
       "       [0.06827787],\n",
       "       [0.3507691 ],\n",
       "       [0.25697088],\n",
       "       [0.0321038 ],\n",
       "       [0.8263805 ],\n",
       "       [0.93800044],\n",
       "       [0.32368186],\n",
       "       [0.11477735],\n",
       "       [0.26173383],\n",
       "       [0.05645354],\n",
       "       [0.63195693],\n",
       "       [0.60808   ],\n",
       "       [0.04968395],\n",
       "       [0.03191806],\n",
       "       [0.730992  ],\n",
       "       [1.0921983 ],\n",
       "       [0.14992362],\n",
       "       [0.47941154],\n",
       "       [0.9603397 ],\n",
       "       [0.1268065 ],\n",
       "       [0.27154416],\n",
       "       [0.12405615],\n",
       "       [0.27191517],\n",
       "       [0.16087815],\n",
       "       [0.1221986 ],\n",
       "       [0.9718585 ],\n",
       "       [0.5901535 ],\n",
       "       [0.14324114],\n",
       "       [0.15674686],\n",
       "       [0.31949845],\n",
       "       [0.6658008 ],\n",
       "       [0.99071354],\n",
       "       [0.5680137 ],\n",
       "       [0.3271799 ],\n",
       "       [0.81983244],\n",
       "       [0.01955518],\n",
       "       [0.9795971 ],\n",
       "       [0.23616807],\n",
       "       [0.07236989],\n",
       "       [0.028248  ],\n",
       "       [0.12924765],\n",
       "       [0.12528801],\n",
       "       [0.03911761],\n",
       "       [0.88135684],\n",
       "       [0.21086818],\n",
       "       [0.03492921],\n",
       "       [0.02478087],\n",
       "       [0.07318257],\n",
       "       [0.7138746 ],\n",
       "       [0.06683421],\n",
       "       [0.44005144],\n",
       "       [0.02608126],\n",
       "       [0.31279272],\n",
       "       [0.2014159 ],\n",
       "       [0.4630056 ],\n",
       "       [0.90239406],\n",
       "       [0.18418902],\n",
       "       [0.84646535],\n",
       "       [0.11841822],\n",
       "       [0.6213845 ],\n",
       "       [0.9888042 ],\n",
       "       [0.89495414],\n",
       "       [0.09593196],\n",
       "       [0.579836  ],\n",
       "       [0.76361334],\n",
       "       [0.7085545 ],\n",
       "       [0.92086816],\n",
       "       [0.7516379 ],\n",
       "       [0.19389543],\n",
       "       [1.0411265 ],\n",
       "       [0.05626254],\n",
       "       [0.04557324],\n",
       "       [0.22060934],\n",
       "       [0.250315  ],\n",
       "       [0.12860602],\n",
       "       [0.7160239 ],\n",
       "       [0.40145004],\n",
       "       [0.28133136],\n",
       "       [0.38247147],\n",
       "       [0.02824051],\n",
       "       [0.08781687],\n",
       "       [0.42358395],\n",
       "       [0.13283914],\n",
       "       [0.10320609],\n",
       "       [0.02321369],\n",
       "       [0.34101292],\n",
       "       [0.8903755 ],\n",
       "       [0.4456705 ],\n",
       "       [0.93009615],\n",
       "       [0.02914754],\n",
       "       [0.737064  ],\n",
       "       [0.20325336],\n",
       "       [0.03748871],\n",
       "       [0.9446138 ],\n",
       "       [0.87915814],\n",
       "       [0.9758738 ],\n",
       "       [0.28133136],\n",
       "       [0.6987013 ],\n",
       "       [0.9877286 ],\n",
       "       [0.9369223 ],\n",
       "       [0.6100784 ],\n",
       "       [0.30881414],\n",
       "       [0.37139228],\n",
       "       [1.0023624 ],\n",
       "       [0.17784737],\n",
       "       [0.02319716],\n",
       "       [0.64204645],\n",
       "       [0.08257328],\n",
       "       [0.48656672],\n",
       "       [0.6843881 ],\n",
       "       [1.0363593 ],\n",
       "       [0.03403098],\n",
       "       [0.70316184],\n",
       "       [0.45236978],\n",
       "       [0.3974516 ],\n",
       "       [0.02256727],\n",
       "       [0.14992362],\n",
       "       [0.34855694],\n",
       "       [0.02441171],\n",
       "       [0.06025631],\n",
       "       [0.46713465],\n",
       "       [0.03116942],\n",
       "       [0.22146168],\n",
       "       [0.3999437 ],\n",
       "       [0.11435805],\n",
       "       [0.04200276],\n",
       "       [0.8655584 ],\n",
       "       [0.12297671],\n",
       "       [0.26903707],\n",
       "       [0.98430634],\n",
       "       [0.0247157 ],\n",
       "       [0.09928532],\n",
       "       [1.0534382 ],\n",
       "       [0.03885769],\n",
       "       [0.2660372 ],\n",
       "       [0.02463743],\n",
       "       [0.03716835],\n",
       "       [0.4859811 ],\n",
       "       [0.07862329],\n",
       "       [0.12279575],\n",
       "       [0.3717    ],\n",
       "       [0.04300757],\n",
       "       [0.21820176],\n",
       "       [0.4514184 ],\n",
       "       [0.5864186 ],\n",
       "       [0.96097493],\n",
       "       [0.7003786 ],\n",
       "       [0.52006805],\n",
       "       [0.02770236],\n",
       "       [0.64122313],\n",
       "       [0.04290513],\n",
       "       [0.56387293],\n",
       "       [0.06537141],\n",
       "       [0.72457707],\n",
       "       [0.9848952 ],\n",
       "       [0.997355  ],\n",
       "       [0.9475554 ],\n",
       "       [0.127335  ],\n",
       "       [0.05140171],\n",
       "       [0.08673544],\n",
       "       [0.931829  ],\n",
       "       [0.6296201 ],\n",
       "       [0.08575603],\n",
       "       [0.98569643],\n",
       "       [0.39383885],\n",
       "       [0.03017925],\n",
       "       [0.20769022],\n",
       "       [0.59012717],\n",
       "       [0.7331948 ],\n",
       "       [0.6703856 ],\n",
       "       [0.6116724 ],\n",
       "       [0.1268065 ],\n",
       "       [0.16350055],\n",
       "       [0.02300499],\n",
       "       [0.38101822],\n",
       "       [0.06708407],\n",
       "       [0.5787531 ],\n",
       "       [1.0132459 ],\n",
       "       [0.39163667],\n",
       "       [0.0264491 ],\n",
       "       [0.98233175],\n",
       "       [0.05241811],\n",
       "       [0.68844426],\n",
       "       [0.0452924 ],\n",
       "       [0.8268383 ],\n",
       "       [0.30854818],\n",
       "       [0.93009615],\n",
       "       [0.7415205 ],\n",
       "       [0.22475295],\n",
       "       [0.03289843],\n",
       "       [0.9730092 ],\n",
       "       [1.0279689 ],\n",
       "       [0.04942824],\n",
       "       [0.6457772 ],\n",
       "       [0.12706886],\n",
       "       [0.05085368],\n",
       "       [0.05054665],\n",
       "       [0.3134209 ],\n",
       "       [0.24400657],\n",
       "       [0.28172728],\n",
       "       [0.5791534 ],\n",
       "       [0.61623454],\n",
       "       [0.2448804 ],\n",
       "       [0.08865182],\n",
       "       [0.35497832],\n",
       "       [0.08397718],\n",
       "       [0.16158766],\n",
       "       [0.44227195],\n",
       "       [0.9175532 ],\n",
       "       [0.0371271 ],\n",
       "       [0.4040301 ],\n",
       "       [0.0771725 ],\n",
       "       [0.02155856],\n",
       "       [0.43582466],\n",
       "       [0.04483413],\n",
       "       [0.68991816],\n",
       "       [0.77478576],\n",
       "       [0.02940229],\n",
       "       [0.31889847],\n",
       "       [0.30143136],\n",
       "       [0.79301083],\n",
       "       [0.93871254],\n",
       "       [0.10010968],\n",
       "       [0.05244576],\n",
       "       [0.4597141 ],\n",
       "       [0.1110604 ],\n",
       "       [0.03748871],\n",
       "       [0.02725981],\n",
       "       [0.049946  ],\n",
       "       [0.5122938 ],\n",
       "       [0.36975598],\n",
       "       [0.4013366 ],\n",
       "       [0.92574006],\n",
       "       [0.51787055],\n",
       "       [0.16452923],\n",
       "       [0.61208993],\n",
       "       [0.05400861],\n",
       "       [0.5537584 ],\n",
       "       [0.6197442 ],\n",
       "       [0.54489416],\n",
       "       [0.02894253],\n",
       "       [0.83273566],\n",
       "       [0.2899211 ],\n",
       "       [0.9293238 ],\n",
       "       [0.83460975],\n",
       "       [0.10895316],\n",
       "       [0.4588595 ],\n",
       "       [0.03050533],\n",
       "       [0.0261202 ],\n",
       "       [0.04210159],\n",
       "       [0.10391815],\n",
       "       [0.02882347],\n",
       "       [0.1842326 ],\n",
       "       [1.021225  ],\n",
       "       [0.06842761],\n",
       "       [0.15944868],\n",
       "       [0.03809702],\n",
       "       [0.03042694],\n",
       "       [0.20797145],\n",
       "       [0.9805268 ],\n",
       "       [0.65823257],\n",
       "       [0.10596704],\n",
       "       [0.28899422],\n",
       "       [0.03841557],\n",
       "       [0.30083758],\n",
       "       [0.05271275],\n",
       "       [0.98623353],\n",
       "       [0.66925925],\n",
       "       [0.35976148],\n",
       "       [0.78642887],\n",
       "       [0.42832333],\n",
       "       [0.02396095],\n",
       "       [0.03044181],\n",
       "       [0.6630806 ],\n",
       "       [0.2625974 ],\n",
       "       [0.02940229],\n",
       "       [0.80089283],\n",
       "       [0.0820443 ],\n",
       "       [0.7644843 ],\n",
       "       [0.04511114],\n",
       "       [0.26962292],\n",
       "       [0.6367843 ],\n",
       "       [0.7956743 ],\n",
       "       [0.29391676],\n",
       "       [0.07109664],\n",
       "       [0.5223404 ],\n",
       "       [0.5443491 ],\n",
       "       [0.44564787],\n",
       "       [0.03017925],\n",
       "       [0.23634885],\n",
       "       [0.3695957 ],\n",
       "       [0.32018715],\n",
       "       [0.71506345],\n",
       "       [0.74129504],\n",
       "       [1.0035712 ],\n",
       "       [0.08231889],\n",
       "       [0.65271336],\n",
       "       [1.0098469 ],\n",
       "       [0.13229525],\n",
       "       [0.02710192],\n",
       "       [0.03496948],\n",
       "       [0.60767245],\n",
       "       [0.13779141],\n",
       "       [0.65312123],\n",
       "       [0.98898464],\n",
       "       [0.08450399],\n",
       "       [0.6339413 ],\n",
       "       [0.02206411],\n",
       "       [0.9770837 ],\n",
       "       [0.10781431],\n",
       "       [0.8985821 ],\n",
       "       [0.11432311],\n",
       "       [0.41551897],\n",
       "       [0.03226609],\n",
       "       [0.6549077 ],\n",
       "       [0.63837403],\n",
       "       [0.25790054],\n",
       "       [0.91864157],\n",
       "       [1.0841544 ],\n",
       "       [0.94605464],\n",
       "       [0.03207408],\n",
       "       [0.26703838],\n",
       "       [0.77972436],\n",
       "       [0.03502152],\n",
       "       [0.02478087],\n",
       "       [0.8702429 ],\n",
       "       [0.02758266],\n",
       "       [0.44617194],\n",
       "       [0.04532342],\n",
       "       [0.49733496],\n",
       "       [0.6812732 ],\n",
       "       [0.20075858],\n",
       "       [0.5301273 ],\n",
       "       [0.45390296],\n",
       "       [0.0771725 ],\n",
       "       [0.22481579],\n",
       "       [0.62079406],\n",
       "       [0.03602363],\n",
       "       [0.6250869 ],\n",
       "       [0.05744086],\n",
       "       [0.41515613],\n",
       "       [0.11708073],\n",
       "       [0.9872967 ],\n",
       "       [0.07382284],\n",
       "       [0.21454556],\n",
       "       [0.5219217 ],\n",
       "       [0.02275174],\n",
       "       [0.03934717],\n",
       "       [0.09505124],\n",
       "       [0.07255663],\n",
       "       [0.93039936],\n",
       "       [0.03587804],\n",
       "       [0.02888318],\n",
       "       [0.18838367],\n",
       "       [0.63054776],\n",
       "       [0.9688848 ],\n",
       "       [0.79760945],\n",
       "       [0.8324882 ],\n",
       "       [0.02393999],\n",
       "       [0.02608126],\n",
       "       [0.13774899],\n",
       "       [0.28899422],\n",
       "       [0.6823062 ],\n",
       "       [0.80873084],\n",
       "       [0.7830683 ],\n",
       "       [0.25773287],\n",
       "       [0.5796662 ],\n",
       "       [0.0827632 ],\n",
       "       [0.18639967],\n",
       "       [0.48276123],\n",
       "       [0.6757235 ],\n",
       "       [0.28933728],\n",
       "       [0.5457995 ],\n",
       "       [0.62972796],\n",
       "       [1.0049458 ],\n",
       "       [0.6009654 ],\n",
       "       [0.5586809 ],\n",
       "       [1.0210371 ],\n",
       "       [0.18554056],\n",
       "       [0.8757858 ],\n",
       "       [0.54348487],\n",
       "       [0.98022974],\n",
       "       [0.2251494 ],\n",
       "       [1.0125452 ],\n",
       "       [0.45060933],\n",
       "       [0.95374584],\n",
       "       [0.8038291 ],\n",
       "       [0.5879154 ],\n",
       "       [0.78610486],\n",
       "       [0.15963447],\n",
       "       [0.8260542 ],\n",
       "       [0.02226189],\n",
       "       [0.66317576],\n",
       "       [0.03066976],\n",
       "       [0.40684682],\n",
       "       [0.8171685 ],\n",
       "       [0.8633285 ],\n",
       "       [0.95206976],\n",
       "       [0.6335498 ],\n",
       "       [0.030529  ],\n",
       "       [0.03065685],\n",
       "       [0.1095265 ],\n",
       "       [0.8520615 ],\n",
       "       [0.7244153 ],\n",
       "       [0.56803817],\n",
       "       [1.004991  ],\n",
       "       [0.94540906],\n",
       "       [0.05011655],\n",
       "       [0.04072662],\n",
       "       [0.06591833],\n",
       "       [0.69370806],\n",
       "       [0.09662388],\n",
       "       [0.55195844],\n",
       "       [0.39705306],\n",
       "       [0.47002044],\n",
       "       [0.44944376],\n",
       "       [0.08018002],\n",
       "       [0.06364129],\n",
       "       [0.0976382 ],\n",
       "       [0.23725148],\n",
       "       [0.6827732 ],\n",
       "       [0.452101  ],\n",
       "       [0.2982754 ],\n",
       "       [0.31912088],\n",
       "       [0.24369402],\n",
       "       [0.55650455],\n",
       "       [0.57913107],\n",
       "       [0.2153599 ],\n",
       "       [1.0021343 ],\n",
       "       [0.5255512 ],\n",
       "       [0.3713637 ],\n",
       "       [0.02345058],\n",
       "       [0.22435468],\n",
       "       [0.12786509],\n",
       "       [0.7380258 ],\n",
       "       [0.95121956],\n",
       "       [0.04228293],\n",
       "       [0.9457863 ],\n",
       "       [0.04532342],\n",
       "       [0.38635126],\n",
       "       [0.7454275 ],\n",
       "       [0.84519744],\n",
       "       [0.16710824],\n",
       "       [0.57589376],\n",
       "       [0.15234452],\n",
       "       [0.78433526],\n",
       "       [0.16742842],\n",
       "       [0.63467336],\n",
       "       [0.44192857],\n",
       "       [0.71891636],\n",
       "       [0.9958442 ],\n",
       "       [0.5279341 ],\n",
       "       [0.16904391],\n",
       "       [0.06353508],\n",
       "       [0.02627822],\n",
       "       [0.37076992],\n",
       "       [0.19834326],\n",
       "       [0.9284799 ],\n",
       "       [0.05395879],\n",
       "       [0.07869622],\n",
       "       [0.6089361 ],\n",
       "       [0.42990738],\n",
       "       [1.1017988 ],\n",
       "       [0.18520257],\n",
       "       [0.03648246],\n",
       "       [0.02518455],\n",
       "       [0.91152316],\n",
       "       [0.8578341 ]], dtype=float32)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12817375392746724\n",
      "0.12780450082647615\n",
      "0.12764293805262492\n",
      "0.1280689212196631\n"
     ]
    }
   ],
   "source": [
    "for d in range(40, 44):\n",
    "    rgr_rfc = RandomForestRegressor(max_depth=d, \n",
    "                                    random_state=42, \n",
    "                                    max_features='auto', \n",
    "                                    criterion='mae',\n",
    "                                    warm_start=True,\n",
    "                                    n_estimators=200)\n",
    "    rgr_rfc.fit(X_train, y_train)\n",
    "    print( mean_squared_error( rgr_rfc.predict(X_test), y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgr_kNN = KNeighborsRegressor(n_neighbors=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
       "                    weights='uniform')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgr_kNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18227112188655142"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((np.abs(rgr_kNN.predict(X_test) - y_test))**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54960317])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgr_kNN.predict([X[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31349206, 0.        , 0.        , 0.        , 0.12698413,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game(screen_x, screen_y, end_line, balls_setting, max_random_ball_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf_rfc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-8ccd44eca0bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf_rfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clf_rfc' is not defined"
     ]
    }
   ],
   "source": [
    "clf_rfc.predict( np.array([game.current_state.vectorize()]) )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_episode(game, agent, max_step = None, plot = False):\n",
    "    is_finish = False\n",
    "    current_state = game.init_state()\n",
    "    reward_recorder = []\n",
    "    step = 1\n",
    "\n",
    "    while not is_finish:\n",
    "        action = agent.get_action(current_state)\n",
    "#         print(action)\n",
    "        next_state, reward, is_finish = game.next_step(action, verbose = False)\n",
    "        if plot:\n",
    "            next_state.plot_state()\n",
    "        reward_recorder.append(reward)\n",
    "        current_state = next_state\n",
    "        step += 1\n",
    "        if max_step and step >= max_step:\n",
    "            break\n",
    "    \n",
    "    return reward_recorder[-1], reward_recorder, game.current_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imitate_Agent(object):\n",
    "    \n",
    "    def __init__(self, model, game):\n",
    "        self.model = model\n",
    "        self.game = game\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        return int( self.game.screen_x * (1* self.model.predict( np.array([state.vectorize()]) )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t th episode:  201\n",
      "1 \t th episode:  238\n",
      "2 \t th episode:  182\n",
      "3 \t th episode:  304\n",
      "4 \t th episode:  142\n",
      "5 \t th episode:  190\n",
      "6 \t th episode:  200\n",
      "7 \t th episode:  218\n",
      "8 \t th episode:  236\n",
      "9 \t th episode:  191\n",
      "10 \t th episode:  162\n",
      "11 \t th episode:  275\n",
      "12 \t th episode:  239\n",
      "13 \t th episode:  236\n",
      "14 \t th episode:  181\n",
      "15 \t th episode:  224\n",
      "16 \t th episode:  235\n",
      "17 \t th episode:  303\n",
      "18 \t th episode:  191\n",
      "19 \t th episode:  270\n",
      "20 \t th episode:  212\n",
      "21 \t th episode:  266\n",
      "22 \t th episode:  246\n",
      "23 \t th episode:  190\n",
      "24 \t th episode:  220\n",
      "25 \t th episode:  223\n",
      "26 \t th episode:  254\n",
      "27 \t th episode:  149\n",
      "28 \t th episode:  134\n",
      "29 \t th episode:  188\n",
      "14.159441725413005\n"
     ]
    }
   ],
   "source": [
    "nn_policy =Imitate_Agent(rgr_rfc, game)\n",
    "\n",
    "final_rewards = [] # sum of the score at each step\n",
    "scores = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(30):\n",
    "    game = Game(screen_x, screen_y, end_line, balls_setting, max_random_ball_level)\n",
    "    R, _, score = play_one_episode(game, nn_policy, max_step=100)\n",
    "    print(i, \"\\t th episode: \", R)\n",
    "    final_rewards.append(R)\n",
    "    scores.append(score)\n",
    "end_time = time.time()\n",
    "\n",
    "print( (end_time - start_time)/ 60 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216.66666666666666"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.70701217468729"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Points')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(scores, density=True, bins=15)\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Points')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
